{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abd745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed df\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "with capture_output():\n",
    "    %run 03_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924c840",
   "metadata": {},
   "source": [
    "# Advanced NLP Models: Neural Network and BERT\n",
    "\n",
    "This notebook trains advanced models including Deep Neural Networks and BERT-based transformers for transaction categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9565ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (969666, 5022)\n",
      "Test set shape: (336786, 5022)\n",
      "Number of classes: 9\n",
      "Class distribution in training set:\n",
      "GENERAL_MERCHANDISE    391492\n",
      "FOOD_AND_BEVERAGES     357992\n",
      "GROCERIES              162754\n",
      "TRAVEL                  41839\n",
      "PETS                     6599\n",
      "EDUCATION                3329\n",
      "RENT                     2518\n",
      "OVERDRAFT                2433\n",
      "MORTGAGE                  710\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "print(f\"Class distribution in training set:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2a0ce",
   "metadata": {},
   "source": [
    "## GPU Configuration\n",
    "\n",
    "Check for GPU availability and configure both TensorFlow and PyTorch to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa75800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU Check:\n",
      "  GPU Available: []\n",
      "  ⚠ No GPU detected for TensorFlow\n",
      "\n",
      "PyTorch GPU Check:\n",
      "  CUDA Available: True\n",
      "  CUDA Device: NVIDIA GeForce RTX 4090 Laptop GPU\n",
      "  CUDA Version: 12.1\n",
      "  ✓ Using device: cuda\n",
      "\n",
      "Device for BERT training: cuda\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# Check TensorFlow GPU\n",
    "print(\"TensorFlow GPU Check:\")\n",
    "print(f\"  GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"  GPU Devices: {[gpu.name for gpu in tf.config.list_physical_devices('GPU')]}\")\n",
    "    # Enable memory growth to avoid allocating all GPU memory at once\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"  ✓ GPU memory growth enabled\")\n",
    "else:\n",
    "    print(\"  ⚠ No GPU detected for TensorFlow\")\n",
    "\n",
    "# Check PyTorch GPU\n",
    "print(\"\\nPyTorch GPU Check:\")\n",
    "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"  ✓ Using device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"  ⚠ CUDA not available, using CPU\")\n",
    "\n",
    "print(f\"\\nDevice for BERT training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bfd961",
   "metadata": {},
   "source": [
    "**Note:** TensorFlow doesn't detect GPU because it needs proper CUDA setup. Since PyTorch detects your RTX 4090, we'll use it for BERT training which will be **significantly faster** than CPU. The Neural Network model can still run efficiently on CPU with the current TensorFlow setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2604b2e",
   "metadata": {},
   "source": [
    "## 1. Neural Network with TensorFlow/Keras\n",
    "\n",
    "A deep learning approach using a neural network with dense layers and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6884575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Neural Network...\n",
      "Input dimension: 5022\n",
      "Number of classes: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,571,776</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m2,571,776\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │         \u001b[38;5;34m1,161\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,737,161</span> (10.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,737,161\u001b[0m (10.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,737,161</span> (10.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,737,161\u001b[0m (10.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# Convert sparse matrices to dense for neural network\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "# Encode labels\n",
    "nn_label_encoder = LabelEncoder()\n",
    "y_train_nn = nn_label_encoder.fit_transform(y_train)\n",
    "y_test_nn = nn_label_encoder.transform(y_test)\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "input_dim = X_train_dense.shape[1]\n",
    "\n",
    "print(f\"Building Neural Network...\")\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "# Build model\n",
    "nn_model = keras.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "nn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67b7933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network...\n",
      "Epoch 1/30\n",
      "\u001b[1m6061/6061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9352 - loss: 0.1832 - val_accuracy: 0.9190 - val_loss: 0.2336\n",
      "Epoch 2/30\n",
      "\u001b[1m6061/6061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 17ms/step - accuracy: 0.9531 - loss: 0.1303 - val_accuracy: 0.9216 - val_loss: 0.2314\n",
      "Epoch 3/30\n",
      "\u001b[1m6061/6061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 16ms/step - accuracy: 0.9562 - loss: 0.1197 - val_accuracy: 0.9231 - val_loss: 0.2306\n",
      "Epoch 4/30\n",
      "\u001b[1m6061/6061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - accuracy: 0.9584 - loss: 0.1131 - val_accuracy: 0.9227 - val_loss: 0.2290\n",
      "Epoch 5/30\n",
      "\u001b[1m6061/6061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 16ms/step - accuracy: 0.9598 - loss: 0.1088 - val_accuracy: 0.9218 - val_loss: 0.2404\n",
      "Epoch 6/30\n",
      "\u001b[1m6061/6061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9609 - loss: 0.1053 - val_accuracy: 0.9228 - val_loss: 0.2374\n",
      "Epoch 7/30\n",
      "\u001b[1m6061/6061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9617 - loss: 0.1027 - val_accuracy: 0.9218 - val_loss: 0.2411\n",
      "Epoch 8/30\n",
      "\u001b[1m6061/6061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9627 - loss: 0.1004 - val_accuracy: 0.9221 - val_loss: 0.2439\n",
      "Epoch 9/30\n",
      "\u001b[1m6061/6061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1877s\u001b[0m 310ms/step - accuracy: 0.9632 - loss: 0.0981 - val_accuracy: 0.9196 - val_loss: 0.2595\n",
      "\n",
      "Training completed in 2778.81 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network\n",
    "print(f\"\\nTraining Neural Network...\")\n",
    "start_time = time.time()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = nn_model.fit(\n",
    "    X_train_dense, y_train_nn,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037e3274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10525/10525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step\n",
      "\n",
      "==================================================\n",
      "Neural Network Results\n",
      "==================================================\n",
      "Accuracy: 0.9315\n",
      "\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "          EDUCATION       0.85      0.49      0.62      1170\n",
      " FOOD_AND_BEVERAGES       0.89      0.96      0.92    124002\n",
      "GENERAL_MERCHANDISE       0.96      0.91      0.93    132571\n",
      "          GROCERIES       0.96      0.93      0.94     56577\n",
      "           MORTGAGE       0.95      0.87      0.91       409\n",
      "          OVERDRAFT       0.99      0.98      0.98       953\n",
      "               PETS       0.99      0.92      0.95      2667\n",
      "               RENT       0.74      0.83      0.78       629\n",
      "             TRAVEL       0.96      0.92      0.94     17808\n",
      "\n",
      "           accuracy                           0.93    336786\n",
      "          macro avg       0.92      0.87      0.89    336786\n",
      "       weighted avg       0.93      0.93      0.93    336786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Neural Network\n",
    "y_pred_nn = nn_model.predict(X_test_dense)\n",
    "y_pred_nn_classes = np.argmax(y_pred_nn, axis=1)\n",
    "y_pred_nn_labels = nn_label_encoder.inverse_transform(y_pred_nn_classes)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Neural Network Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_nn_labels):.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89ab0a",
   "metadata": {},
   "source": [
    "## 2. BERT-based Models\n",
    "\n",
    "Let's try fine-tuning pre-trained BERT models for transaction categorization. We'll use DistilBERT (a lighter version) for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8736e518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT training samples: 50000\n",
      "BERT test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Prepare data for BERT - need the original text with categories\n",
    "# Get from outflows_cleaned which has both memo_clean and category\n",
    "bert_train_df = outflows_cleaned[outflows_cleaned['prism_consumer_id'].isin(outflows_train_ids)].copy()\n",
    "bert_test_df = outflows_cleaned[outflows_cleaned['prism_consumer_id'].isin(outflows_test_ids)].copy()\n",
    "\n",
    "# Use a subset for faster training\n",
    "sample_size = 50000\n",
    "bert_train_df = bert_train_df.sample(n=min(sample_size, len(bert_train_df)), random_state=42)\n",
    "bert_test_df = bert_test_df.sample(n=min(10000, len(bert_test_df)), random_state=42)\n",
    "\n",
    "print(f\"BERT training samples: {len(bert_train_df)}\")\n",
    "print(f\"BERT test samples: {len(bert_test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a608d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "  EDUCATION: 0\n",
      "  FOOD_AND_BEVERAGES: 1\n",
      "  GENERAL_MERCHANDISE: 2\n",
      "  GROCERIES: 3\n",
      "  MORTGAGE: 4\n",
      "  OVERDRAFT: 5\n",
      "  PETS: 6\n",
      "  RENT: 7\n",
      "  TRAVEL: 8\n"
     ]
    }
   ],
   "source": [
    "# Create label mapping\n",
    "unique_labels = sorted(bert_train_df['category'].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Label mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {label}: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf3c026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50000\n",
      "})\n",
      "Test dataset: Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets\n",
    "bert_train_dataset = Dataset.from_dict({\n",
    "    'text': bert_train_df['memo_clean'].tolist(),\n",
    "    'label': [label2id[label] for label in bert_train_df['category'].tolist()]\n",
    "})\n",
    "\n",
    "bert_test_dataset = Dataset.from_dict({\n",
    "    'text': bert_test_df['memo_clean'].tolist(),\n",
    "    'label': [label2id[label] for label in bert_test_df['category'].tolist()]\n",
    "})\n",
    "\n",
    "print(f\"Train dataset: {bert_train_dataset}\")\n",
    "print(f\"Test dataset: {bert_test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70a1a5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb866721b9e4fbc95fe60d7496c9fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kangy\\miniconda3\\envs\\dsc180-prism\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kangy\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fac55300214a62b919a7f7f85ea8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7b98c3344347e5ae417ea5848292f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a40ac57b83d4c659a5ebc2f1176355f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b7e973b5484ea689c1f8ce2b2db277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37db2cea4734b75941dc5627d7b0da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets tokenized successfully\n"
     ]
    }
   ],
   "source": [
    "# Load DistilBERT tokenizer and tokenize datasets\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "bert_train_tokenized = bert_train_dataset.map(tokenize_function, batched=True)\n",
    "bert_test_tokenized = bert_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Datasets tokenized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb319e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a146aafd7d204b0cb2fc3786bb32ae59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded with 9 labels and moved to GPU\n"
     ]
    }
   ],
   "source": [
    "# Load DistilBERT model and move to GPU if available\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    bert_model = bert_model.to(device)\n",
    "    print(f\"✓ Model loaded with {len(unique_labels)} labels and moved to GPU\")\n",
    "else:\n",
    "    print(f\"Model loaded with {len(unique_labels)} labels (CPU)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f0db5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training configured to use GPU with FP16\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments with GPU optimization\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32 if torch.cuda.is_available() else 16,  # Larger batch on GPU\n",
    "    per_device_eval_batch_size=32 if torch.cuda.is_available() else 16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),  # Enable mixed precision on GPU\n",
    "    dataloader_num_workers=0,  # Windows compatibility\n",
    "    no_cuda=not torch.cuda.is_available(),  # Use GPU if available\n",
    ")\n",
    "\n",
    "print(f\"✓ Training configured to use {'GPU with FP16' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c64d02b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics function defined\n"
     ]
    }
   ],
   "source": [
    "# Define compute metrics function\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "print(\"Metrics function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "529b02fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BERT training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kangy\\miniconda3\\envs\\dsc180-prism\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:392: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4689/4689 05:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.205600</td>\n",
       "      <td>0.222107</td>\n",
       "      <td>0.927400</td>\n",
       "      <td>0.926494</td>\n",
       "      <td>0.925800</td>\n",
       "      <td>0.927400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>0.232667</td>\n",
       "      <td>0.934000</td>\n",
       "      <td>0.933459</td>\n",
       "      <td>0.934482</td>\n",
       "      <td>0.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.082800</td>\n",
       "      <td>0.249376</td>\n",
       "      <td>0.936200</td>\n",
       "      <td>0.935752</td>\n",
       "      <td>0.936758</td>\n",
       "      <td>0.936200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kangy\\miniconda3\\envs\\dsc180-prism\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT training completed in 317.76 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create Trainer and train the model\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=bert_train_tokenized,\n",
    "    eval_dataset=bert_test_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting BERT training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "bert_training_time = time.time() - start_time\n",
    "print(f\"\\nBERT training completed in {bert_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3113ffda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT model on full test set...\n",
      "\n",
      "==================================================\n",
      "BERT (DistilBERT) Results\n",
      "==================================================\n",
      "Accuracy: 0.9362\n",
      "F1 Score: 0.9358\n",
      "Precision: 0.9368\n",
      "Recall: 0.9362\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BERT on full test set\n",
    "print(\"Evaluating BERT model on full test set...\")\n",
    "bert_eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BERT (DistilBERT) Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {bert_eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {bert_eval_results['eval_f1']:.4f}\")\n",
    "print(f\"Precision: {bert_eval_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {bert_eval_results['eval_recall']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5317ba8",
   "metadata": {},
   "source": [
    "## 3. RoBERTa Model\n",
    "\n",
    "RoBERTa is an optimized version of BERT with improved training methodology, often performing better on classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c3720a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b20a447fd4c4e30b1d2ae50fa06341e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kangy\\miniconda3\\envs\\dsc180-prism\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kangy\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e761fe6ecafb4ed38a433239ec05ff4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b8ee2afa434a6eb0eca664108352e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfcca2e7c3b4569abdf361f685b6c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b9edd5169c4a68a1b90456b5f5d22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7964e04b123549d9ab1ebedbb8bea49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285a285df52c4f5f8dd2d9130cb02afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa datasets tokenized successfully\n"
     ]
    }
   ],
   "source": [
    "# Load RoBERTa model\n",
    "roberta_model_name = \"roberta-base\"\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
    "\n",
    "# Tokenize datasets for RoBERTa\n",
    "def roberta_tokenize_function(examples):\n",
    "    return roberta_tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "roberta_train_tokenized = bert_train_dataset.map(roberta_tokenize_function, batched=True)\n",
    "roberta_test_tokenized = bert_test_dataset.map(roberta_tokenize_function, batched=True)\n",
    "\n",
    "print(\"RoBERTa datasets tokenized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee41022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232499b1c34b4d99923efb19e4901f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RoBERTa loaded with 9 labels and moved to GPU\n"
     ]
    }
   ],
   "source": [
    "# Load RoBERTa model for sequence classification\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    roberta_model_name,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    roberta_model = roberta_model.to(device)\n",
    "    print(f\"✓ RoBERTa loaded with {len(unique_labels)} labels and moved to GPU\")\n",
    "else:\n",
    "    print(f\"RoBERTa loaded with {len(unique_labels)} labels (CPU)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d0dd1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RoBERTa training configured to use GPU with FP16\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments for RoBERTa\n",
    "roberta_training_args = TrainingArguments(\n",
    "    output_dir=\"./results_roberta\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32 if torch.cuda.is_available() else 16,\n",
    "    per_device_eval_batch_size=32 if torch.cuda.is_available() else 16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir='./logs_roberta',\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "print(f\"✓ RoBERTa training configured to use {'GPU with FP16' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b7c1e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RoBERTa training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4689/4689 07:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>0.297080</td>\n",
       "      <td>0.908700</td>\n",
       "      <td>0.907777</td>\n",
       "      <td>0.907644</td>\n",
       "      <td>0.908700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.205800</td>\n",
       "      <td>0.282049</td>\n",
       "      <td>0.917300</td>\n",
       "      <td>0.916942</td>\n",
       "      <td>0.917570</td>\n",
       "      <td>0.917300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.273245</td>\n",
       "      <td>0.922100</td>\n",
       "      <td>0.922129</td>\n",
       "      <td>0.922351</td>\n",
       "      <td>0.922100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kangy\\miniconda3\\envs\\dsc180-prism\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RoBERTa training completed in 477.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create Trainer and train RoBERTa\n",
    "roberta_trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=roberta_training_args,\n",
    "    train_dataset=roberta_train_tokenized,\n",
    "    eval_dataset=roberta_test_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting RoBERTa training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "roberta_trainer.train()\n",
    "\n",
    "roberta_training_time = time.time() - start_time\n",
    "print(f\"\\nRoBERTa training completed in {roberta_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "459fc793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RoBERTa model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RoBERTa Results\n",
      "==================================================\n",
      "Accuracy: 0.9221\n",
      "F1 Score: 0.9221\n",
      "Precision: 0.9224\n",
      "Recall: 0.9221\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RoBERTa\n",
    "print(\"Evaluating RoBERTa model...\")\n",
    "roberta_eval_results = roberta_trainer.evaluate()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RoBERTa Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {roberta_eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {roberta_eval_results['eval_f1']:.4f}\")\n",
    "print(f\"Precision: {roberta_eval_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {roberta_eval_results['eval_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d47ae8",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare all three models side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67929297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model Comparison Summary\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT (DistilBERT)</td>\n",
       "      <td>0.936200</td>\n",
       "      <td>0.935752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.931535</td>\n",
       "      <td>0.931525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>0.922100</td>\n",
       "      <td>0.922129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Accuracy  F1 Score\n",
       "0  BERT (DistilBERT)  0.936200  0.935752\n",
       "1     Neural Network  0.931535  0.931525\n",
       "2            RoBERTa  0.922100  0.922129"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Comparison\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Model Comparison Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Neural Network', 'BERT (DistilBERT)', 'RoBERTa'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_nn_labels),\n",
    "        bert_eval_results['eval_accuracy'],\n",
    "        roberta_eval_results['eval_accuracy']\n",
    "    ],\n",
    "    'F1 Score': [\n",
    "        precision_recall_fscore_support(y_test, y_pred_nn_labels, average='weighted')[2],\n",
    "        bert_eval_results['eval_f1'],\n",
    "        roberta_eval_results['eval_f1']\n",
    "    ]\n",
    "})\n",
    "\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06862d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

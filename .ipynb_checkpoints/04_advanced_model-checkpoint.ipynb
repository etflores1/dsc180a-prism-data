{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e8148-6d78-4590-884e-c48e34f40b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed df\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "with capture_output():\n",
    "    %run 03_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda6067-c2ed-4cb3-ae73-6324b41a2949",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2d5ed-f3f2-4b8a-8586-30fb03665c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PyTorch MLP for sparse features (densified per batch) ===\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# --- Reproducibility & device ---\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- Label encoding (y_train/y_test are strings) ---\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train)\n",
    "y_test_int  = le.transform(y_test)\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "# --- Train/Val split (from training set) ---\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(X_train.shape[0]),\n",
    "    test_size=0.10,\n",
    "    random_state=SEED,\n",
    "    stratify=y_train_int\n",
    ")\n",
    "X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "y_tr, y_val = y_train_int[train_idx], y_train_int[val_idx]\n",
    "\n",
    "# --- Class weights (to handle imbalance) ---\n",
    "counts = np.bincount(y_tr, minlength=n_classes)\n",
    "class_weights = counts.sum() / np.maximum(counts, 1)\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "\n",
    "# --- Batch generator that densifies CSR per batch (memory-safe) ---\n",
    "def csr_batch_generator(X_csr, y_arr, batch_size=1024, shuffle=True, device=device):\n",
    "    n = X_csr.shape[0]\n",
    "    idx = np.arange(n)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx)\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            b = idx[start:end]\n",
    "            Xb = X_csr[b].toarray().astype(np.float32)\n",
    "            yb = y_arr[b]\n",
    "            yield torch.from_numpy(Xb).to(device), torch.from_numpy(yb).long().to(device)\n",
    "\n",
    "# --- Simple, strong MLP ---\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden=(512, 256), dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bn0 = nn.BatchNorm1d(in_dim)\n",
    "        self.fc1 = nn.Linear(in_dim, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.out = nn.Linear(hidden[1], out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BatchNorm expects dense; we already densify per batch\n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.out(x)\n",
    "\n",
    "model = MLP(input_dim, n_classes, hidden=(512, 256), dropout=0.3).to(device)\n",
    "\n",
    "# --- Optimizer & loss ---\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_t)\n",
    "\n",
    "# --- Training/validation loaders (generators) ---\n",
    "BATCH_SIZE = 1024\n",
    "train_gen = csr_batch_generator(X_tr,  y_tr,  batch_size=BATCH_SIZE, shuffle=True,  device=device)\n",
    "val_gen   = csr_batch_generator(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False, device=device)\n",
    "\n",
    "steps_per_epoch = int(np.ceil(X_tr.shape[0]  / BATCH_SIZE))\n",
    "val_steps = int(np.ceil(X_val.shape[0] / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85623a4c-7ea7-4b6b-91a2-0e11f543bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "EPOCHS = 12\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "patience = 3\n",
    "since_improve = 0\n",
    "\n",
    "def run_epoch(gen, steps, train=True):\n",
    "    model.train(mode=train)\n",
    "    total_loss, total_correct, total_n = 0.0, 0, 0\n",
    "    for _ in range(steps):\n",
    "        xb, yb = next(gen)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == yb).sum().item()\n",
    "            total_n += yb.size(0)\n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "    return total_loss / max(total_n, 1), total_correct / max(total_n, 1)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = run_epoch(train_gen, steps_per_epoch, train=True)\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = run_epoch(val_gen, val_steps, train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} | \"\n",
    "          f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "\n",
    "    # Early stopping on best val_acc\n",
    "    if val_acc > best_val_acc + 1e-4:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "        since_improve = 0\n",
    "    else:\n",
    "        since_improve += 1\n",
    "        if since_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best val_acc={best_val_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba0398-9de6-491e-80ce-24696cb49264",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec5687-381f-4c10-a323-fedd0c477038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def predict_in_batches(X_csr, batch_size=1024):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, X_csr.shape[0], batch_size):\n",
    "            end = min(start + batch_size, X_csr.shape[0])\n",
    "            Xb = torch.from_numpy(X_csr[start:end].toarray().astype(np.float32)).to(device)\n",
    "            logits = model(Xb)\n",
    "            preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "    return np.concatenate(preds, axis=0)\n",
    "\n",
    "# Train/Val/Test predictions\n",
    "y_tr_pred  = predict_in_batches(X_tr)\n",
    "y_val_pred = predict_in_batches(X_val)\n",
    "y_te_pred  = predict_in_batches(X_test)\n",
    "\n",
    "# Accuracies\n",
    "train_acc = accuracy_score(y_tr,  y_tr_pred)\n",
    "val_acc   = accuracy_score(y_val, y_val_pred)\n",
    "test_acc  = accuracy_score(y_test_int, y_te_pred)\n",
    "\n",
    "print(f\"Train Acc: {train_acc:.4f}\")\n",
    "print(f\"Val   Acc: {val_acc:.4f}\")\n",
    "print(f\"Test  Acc: {test_acc:.4f}\")\n",
    "\n",
    "# Detailed metrics (macro-F1 is important for imbalance)\n",
    "print(\"\\n=== Classification Report (TEST) ===\")\n",
    "print(classification_report(y_test_int, y_te_pred, target_names=le.classes_, digits=4))\n",
    "\n",
    "# Confusion matrix (TEST)\n",
    "cm = confusion_matrix(y_test_int, y_te_pred)\n",
    "cm_df = pd.DataFrame(cm, index=[f\"true_{c}\" for c in le.classes_],\n",
    "                        columns=[f\"pred_{c}\" for c in le.classes_])\n",
    "display(cm_df.head(12))  # show top rows if many classes\n",
    "\n",
    "# (Optional) Save label mapping for deployment\n",
    "label_map = {i: cls for i, cls in enumerate(le.classes_)}\n",
    "print(\"\\nLabel map:\", label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b922745-f302-44cb-9a10-28c6e0dbfe38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

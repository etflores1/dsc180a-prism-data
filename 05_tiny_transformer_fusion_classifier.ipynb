{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca0e8148-6d78-4590-884e-c48e34f40b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed df\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "with capture_output():\n",
    "    %run 03_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b3f7f9-46d1-436f-bb1e-be0651de5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # 0 = all, 3 = only fatal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda6067-c2ed-4cb3-ae73-6324b41a2949",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "- Encode labels for model training.\n",
    "- Reduce TF-IDF dimensionality using random projection.\n",
    "- Convert numeric features to dense float tensors.\n",
    "- Tokenize memo text for the tiny transformer.\n",
    "- Build PyTorch datasets and dataloaders combining text, TF-IDF, and numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1018fc8-24da-46b5-971f-c6988477d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': 'cuda', 'n_train': 969666, 'n_test': 336786, 'num_labels': 9, 'num_dim': 22, 'tfidf_proj_dim': 256, 'tfidf_projection_time_sec': 0.33}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 1 — Setup (fixed projection conversion)\n",
    "# =========================\n",
    "import time, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# Labels\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc  = le.transform(y_test)\n",
    "num_labels  = len(le.classes_)\n",
    "\n",
    "# Project TF-IDF to low-dim dense (keeps signal, speeds training)\n",
    "TFIDF_PROJ_DIM = 256\n",
    "srp = SparseRandomProjection(n_components=TFIDF_PROJ_DIM, random_state=42)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "X_train_tfidf_proj = srp.fit_transform(X_train_tfidf)  # may be sparse\n",
    "X_test_tfidf_proj  = srp.transform(X_test_tfidf)\n",
    "tfidf_proj_time = time.perf_counter() - t0\n",
    "\n",
    "def to_dense32(x):\n",
    "    if issparse(x):\n",
    "        x = x.toarray()\n",
    "    return np.asarray(x, dtype=np.float32)\n",
    "\n",
    "# Convert everything to dense float32\n",
    "X_train_tfidf_proj = to_dense32(X_train_tfidf_proj)\n",
    "X_test_tfidf_proj  = to_dense32(X_test_tfidf_proj)\n",
    "X_train_num_d      = to_dense32(X_train_num)\n",
    "X_test_num_d       = to_dense32(X_test_num)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "MODEL_NAME = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "MAX_LEN    = 96\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class TxnDataset(Dataset):\n",
    "    def __init__(self, texts, y, num_feats, tfidf_proj):\n",
    "        self.texts = list(texts)\n",
    "        self.y     = np.asarray(y, dtype=np.int64)\n",
    "        self.num   = np.asarray(num_feats, dtype=np.float32)\n",
    "        self.tfp   = np.asarray(tfidf_proj, dtype=np.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        enc = tokenizer(\n",
    "            self.texts[i],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        batch[\"num\"] = torch.from_numpy(self.num[i])\n",
    "        batch[\"tfp\"] = torch.from_numpy(self.tfp[i])\n",
    "        y = torch.tensor(self.y[i], dtype=torch.long)\n",
    "        return batch, y\n",
    "\n",
    "train_ds = TxnDataset(train_df[\"memo_clean\"], y_train_enc, X_train_num_d, X_train_tfidf_proj)\n",
    "test_ds  = TxnDataset(test_df[\"memo_clean\"],  y_test_enc,  X_test_num_d,  X_test_tfidf_proj)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print({\n",
    "    \"device\": str(DEVICE),\n",
    "    \"n_train\": len(train_ds),\n",
    "    \"n_test\": len(test_ds),\n",
    "    \"num_labels\": num_labels,\n",
    "    \"num_dim\": X_train_num_d.shape[1],\n",
    "    \"tfidf_proj_dim\": TFIDF_PROJ_DIM,\n",
    "    \"tfidf_projection_time_sec\": round(tfidf_proj_time, 2)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5843dbad-eb70-40df-93db-1f734464878d",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "- Use frozen tiny BERT (bert-tiny) for semantic text encoding.\n",
    "- Process numeric engineered features through a small MLP.\n",
    "- Transform projected TF-IDF vectors with another MLP.\n",
    "- Concatenate all feature streams into a fused representation.\n",
    "- Apply a lightweight classifier head for category prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106f79eb-aaa2-414b-99ce-0af18faf903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready. {'backbone_frozen': True, 'epochs': 3}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 2 — Model creation (tiny Transformer + numeric + TF-IDF projection)\n",
    "# =========================\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, get_linear_schedule_with_warmup\n",
    "\n",
    "NUM_DIM = X_train_num_d.shape[1]\n",
    "TFP_DIM = X_train_tfidf_proj.shape[1]\n",
    "\n",
    "class TinyFusion(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        hid = self.backbone.config.hidden_size  # 128 for bert-tiny\n",
    "\n",
    "        # Freeze backbone for speed (uncomment next 4 lines to allow light finetune on last layer)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.num_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(NUM_DIM),\n",
    "            nn.Linear(NUM_DIM, 64), nn.GELU(),\n",
    "        )\n",
    "        self.tfp_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(TFP_DIM),\n",
    "            nn.Linear(TFP_DIM, 128), nn.GELU(),\n",
    "        )\n",
    "        fused_in = hid + 64 + 128\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(fused_in, 256), nn.GELU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        out = self.backbone(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).last_hidden_state\n",
    "        cls = out[:, 0]  # first token representation\n",
    "        num = self.num_mlp(batch[\"num\"])\n",
    "        tfp = self.tfp_mlp(batch[\"tfp\"])\n",
    "        fused = torch.cat([cls, num, tfp], dim=1)\n",
    "        return self.head(fused)\n",
    "\n",
    "model = TinyFusion(num_labels=num_labels).to(DEVICE)\n",
    "\n",
    "# Loss (optional class weights for imbalance)\n",
    "import numpy as np, torch\n",
    "counts = np.bincount(y_train_enc, minlength=num_labels)\n",
    "weights = (1.0 / (counts + 1e-9))\n",
    "weights = weights / weights.sum() * num_labels\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "# Optimizer/scheduler (only head and small MLPs train → fast)\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=2e-3, weight_decay=1e-2)\n",
    "EPOCHS = 3\n",
    "total_steps = EPOCHS * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, int(0.1*total_steps), total_steps)\n",
    "\n",
    "print(\"Model ready.\", {\"backbone_frozen\": True, \"epochs\": EPOCHS})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba0398-9de6-491e-80ce-24696cb49264",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "- Train only the small fusion and MLP layers for speed.\n",
    "- Track loss, accuracy, and F1-macro during training.\n",
    "- Measure training time and inference latency.\n",
    "- Evaluate the model on both train and test sets.\n",
    "- Output metrics and classification performance summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b1d420d-8a65-46b8-bc31-4d187fc966a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train_loss 0.4943 acc 0.8042 f1_macro 0.5891\n",
      "epoch 02 | train_loss 0.2998 acc 0.8697 f1_macro 0.6860\n",
      "epoch 03 | train_loss 0.2292 acc 0.8930 f1_macro 0.7444\n",
      "training_time_sec: 516.18\n",
      "[train] n=969666 acc=0.9229 f1_macro=0.8218 f1_weighted=0.9272 latency=0.16 ms/sample throughput=6272.6 samples/s\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "          EDUCATION     0.1994    0.9625    0.3303      3329\n",
      " FOOD_AND_BEVERAGES     0.9182    0.9261    0.9221    357992\n",
      "GENERAL_MERCHANDISE     0.9669    0.8964    0.9303    391492\n",
      "          GROCERIES     0.9376    0.9653    0.9512    162754\n",
      "           MORTGAGE     0.7615    0.9986    0.8641       710\n",
      "          OVERDRAFT     0.9325    0.9992    0.9647      2433\n",
      "               PETS     0.6880    0.9803    0.8086      6599\n",
      "               RENT     0.5403    0.9813    0.6969      2518\n",
      "             TRAVEL     0.9005    0.9572    0.9280     41839\n",
      "\n",
      "           accuracy                         0.9229    969666\n",
      "          macro avg     0.7606    0.9630    0.8218    969666\n",
      "       weighted avg     0.9352    0.9229    0.9272    969666\n",
      "\n",
      "\n",
      "train confusion matrix (rows = true, cols = predicted):\n",
      "                     EDUCATION  FOOD_AND_BEVERAGES  GENERAL_MERCHANDISE  \\\n",
      "EDUCATION                 3204                  73                   34   \n",
      "FOOD_AND_BEVERAGES        6138              331537                10624   \n",
      "GENERAL_MERCHANDISE       6192               24263               350949   \n",
      "GROCERIES                  252                3974                 1058   \n",
      "MORTGAGE                     0                   0                    1   \n",
      "OVERDRAFT                    1                   1                    0   \n",
      "PETS                        19                  76                   22   \n",
      "RENT                        19                  12                   12   \n",
      "TRAVEL                     244                1150                  269   \n",
      "\n",
      "                     GROCERIES  MORTGAGE  OVERDRAFT  PETS  RENT  TRAVEL  \n",
      "EDUCATION                    2         1          0     6     4       5  \n",
      "FOOD_AND_BEVERAGES        4251        46         90  1723   783    2800  \n",
      "GENERAL_MERCHANDISE       6156       131         63  1072  1167    1499  \n",
      "GROCERIES               157099        42         21   118    82     108  \n",
      "MORTGAGE                     0       709          0     0     0       0  \n",
      "OVERDRAFT                    0         0       2431     0     0       0  \n",
      "PETS                         5         0          0  6469     0       8  \n",
      "RENT                         0         0          0     1  2471       3  \n",
      "TRAVEL                      44         2          2    13    66   40049  \n",
      "[test] n=336786 acc=0.8902 f1_macro=0.7639 f1_weighted=0.8968 latency=0.15 ms/sample throughput=6577.6 samples/s\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "          EDUCATION     0.1473    0.8590    0.2515      1170\n",
      " FOOD_AND_BEVERAGES     0.8835    0.8987    0.8910    124002\n",
      "GENERAL_MERCHANDISE     0.9430    0.8698    0.9049    132571\n",
      "          GROCERIES     0.9126    0.9059    0.9093     56577\n",
      "           MORTGAGE     0.8337    0.8460    0.8398       409\n",
      "          OVERDRAFT     0.9452    0.9948    0.9693       953\n",
      "               PETS     0.6520    0.9310    0.7669      2667\n",
      "               RENT     0.2941    0.8029    0.4305       629\n",
      "             TRAVEL     0.8963    0.9281    0.9119     17808\n",
      "\n",
      "           accuracy                         0.8902    336786\n",
      "          macro avg     0.7231    0.8929    0.7639    336786\n",
      "       weighted avg     0.9071    0.8902    0.8968    336786\n",
      "\n",
      "\n",
      "test confusion matrix (rows = true, cols = predicted):\n",
      "                     EDUCATION  FOOD_AND_BEVERAGES  GENERAL_MERCHANDISE  \\\n",
      "EDUCATION                 1005                  96                   50   \n",
      "FOOD_AND_BEVERAGES        2743              111442                 5324   \n",
      "GENERAL_MERCHANDISE       2657               10253               115310   \n",
      "GROCERIES                  240                3487                 1294   \n",
      "MORTGAGE                     0                  52                    0   \n",
      "OVERDRAFT                    1                   0                    4   \n",
      "PETS                         8                 104                   54   \n",
      "RENT                         8                  63                   33   \n",
      "TRAVEL                     159                 643                  213   \n",
      "\n",
      "                     GROCERIES  MORTGAGE  OVERDRAFT  PETS  RENT  TRAVEL  \n",
      "EDUCATION                    3         1          0     0     7       8  \n",
      "FOOD_AND_BEVERAGES        2023         7         10   720   488    1245  \n",
      "GENERAL_MERCHANDISE       2782        54         32   493   482     508  \n",
      "GROCERIES                51254         6          3    82    85     126  \n",
      "MORTGAGE                     0       346          0     0     0      11  \n",
      "OVERDRAFT                    0         0        948     0     0       0  \n",
      "PETS                        16         0          0  2483     1       1  \n",
      "RENT                         0         0          1     6   505      13  \n",
      "TRAVEL                      82         1          9    24   149   16528  \n",
      "{'tfidf_projection_time_sec': 0.33, 'training_time_sec': 516.18, 'train_acc': 0.9229, 'test_acc': 0.8902, 'test_f1_macro': 0.7639, 'test_latency_ms_per_sample': 0.15}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3 — Train, evaluate, report latency + confusion matrices\n",
    "# =========================\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np, time, torch, pandas as pd\n",
    "\n",
    "def epoch_pass(loader, train=False):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss, n = 0.0, 0\n",
    "    preds, trues = [], []\n",
    "    scaler = GradScaler(enabled=(DEVICE.type==\"cuda\" and train))\n",
    "    for batch, y in loader:\n",
    "        for k in [\"input_ids\",\"attention_mask\",\"num\",\"tfp\"]:\n",
    "            batch[k] = batch[k].to(DEVICE, non_blocking=True)\n",
    "        y = y.to(DEVICE, non_blocking=True)\n",
    "        with autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "            logits = model(batch)\n",
    "            loss = criterion(logits, y)\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if DEVICE.type==\"cuda\":\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "            else:\n",
    "                loss.backward(); optimizer.step()\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item() * y.size(0); n += y.size(0)\n",
    "        preds.extend(torch.argmax(logits,1).detach().cpu().numpy())\n",
    "        trues.extend(y.detach().cpu().numpy())\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    f1m = f1_score(trues, preds, average=\"macro\")\n",
    "    f1w = f1_score(trues, preds, average=\"weighted\")\n",
    "    return total_loss/max(n,1), acc, f1m, f1w, np.array(trues), np.array(preds)\n",
    "\n",
    "t_train = time.perf_counter()\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc, tr_f1m, tr_f1w, _, _ = epoch_pass(train_loader, train=True)\n",
    "    print(f\"epoch {ep:02d} | train_loss {tr_loss:.4f} acc {tr_acc:.4f} f1_macro {tr_f1m:.4f}\")\n",
    "train_time = time.perf_counter() - t_train\n",
    "print(f\"training_time_sec: {train_time:.2f}\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_latency_and_cm(loader, name):\n",
    "    t0 = time.perf_counter()\n",
    "    loss, acc, f1m, f1w, y_true, y_pred = epoch_pass(loader, train=False)\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    n = len(y_true)\n",
    "    lat_ms = (elapsed/max(n,1))*1000.0\n",
    "    tps = n/max(elapsed,1e-9)\n",
    "    print(f\"[{name}] n={n} acc={acc:.4f} f1_macro={f1m:.4f} f1_weighted={f1w:.4f} \"\n",
    "          f\"latency={lat_ms:.2f} ms/sample throughput={tps:.1f} samples/s\")\n",
    "    print(\"\\n\" + classification_report(y_true, y_pred, target_names=le.classes_, digits=4))\n",
    "\n",
    "    # confusion matrix for each class\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(num_labels))\n",
    "    cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "    print(f\"\\n{name} confusion matrix (rows = true, cols = predicted):\")\n",
    "    print(cm_df)\n",
    "\n",
    "    return acc, f1m, f1w, lat_ms, cm_df\n",
    "\n",
    "train_acc, train_f1m, train_f1w, train_lat, train_cm_df = eval_latency_and_cm(train_loader, \"train\")\n",
    "test_acc,  test_f1m,  test_f1w,  test_lat,  test_cm_df  = eval_latency_and_cm(test_loader,  \"test\")\n",
    "\n",
    "summary = {\n",
    "    \"tfidf_projection_time_sec\": round(tfidf_proj_time, 2),\n",
    "    \"training_time_sec\": round(train_time, 2),\n",
    "    \"train_acc\": round(train_acc,4),\n",
    "    \"test_acc\":  round(test_acc,4),\n",
    "    \"test_f1_macro\": round(test_f1m,4),\n",
    "    \"test_latency_ms_per_sample\": round(test_lat,2)\n",
    "}\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32760e-173e-42d6-9c43-f3fe793e89f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
